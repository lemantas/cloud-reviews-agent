from typing import Optional, Dict, List, Any, Tuple, Iterator
from retrieval import retrieve_documents, format_snippets_to_text
from prompts import get_rag_prompt, get_agent_prompt
from clients import get_llm
from langchain_core.messages import HumanMessage, ToolMessage, AIMessage
from graph import get_agent_graph
import uuid

def rag_chain_stream(
    question: str,
    chunk_type: str = "sentence",
    vendor: Optional[str] = None,
    top_k: Optional[int] = None,
    fetch_k: Optional[int] = None
) -> Iterator[str]:
    """Streaming RAG chain to answer questions using retrieved reviews from vector database.

    Args:
        question: User's question to answer
        chunk_type: Type of chunks to retrieve ("sentence" or "review")
        vendor: Optional vendor filter for retrieval
        top_k: Number of results to return (defaults to TOP_K constant)
        fetch_k: Number of candidates for MMR (defaults to FETCH_K constant)

    Yields:
        Text chunks as they are generated by the LLM

    Returns:
        Dictionary with snippets metadata
    """
    prompt = get_rag_prompt()
    llm = get_llm()

    try:
        snippets = retrieve_documents(question, chunk_type, vendor, top_k, fetch_k)
        context = format_snippets_to_text(snippets)

        formatted_prompt = prompt.format(context=context, question=question)

        # Stream the LLM response
        for chunk in llm.stream(formatted_prompt):
            if hasattr(chunk, 'content'):
                yield chunk.content

        # Return metadata for UI to access
        return {"snippets": snippets}
    except Exception as e:
        yield f"Error processing question: {str(e)}"
        return {"snippets": []}

def simple_rag_response(
    question: str,
    chunk_type: str = "sentence",
    vendor: Optional[str] = None,
    top_k: Optional[int] = None,
    fetch_k: Optional[int] = None,
    conversation_history: Optional[List] = None
) -> Iterator[str]:
    """Execute streaming simple RAG workflow without agentic reasoning.

    Uses basic retrieval and LLM generation without tool calling or multi-step reasoning.
    Conversation history parameter is accepted but not used (for API compatibility).

    Args:
        question: User's question to answer
        chunk_type: Type of chunks to retrieve ("sentence" or "review")
        vendor: Optional vendor filter for retrieval
        top_k: Number of results to return
        fetch_k: Number of candidates for MMR
        conversation_history: Deprecated parameter (not used)

    Yields:
        Text chunks as they are generated

    Returns:
        Dictionary with keys: tool_outputs (list), snippets (list)
    """
    try:
        # For simple RAG, we don't use full conversation history in LLM call
        stream_gen = rag_chain_stream(question, chunk_type, vendor, top_k, fetch_k)

        # Yield all text chunks and capture return value
        metadata = {"snippets": []}
        try:
            while True:
                chunk = next(stream_gen)
                yield chunk
        except StopIteration as e:
            # Capture metadata from generator return value
            if e.value:
                metadata = e.value

        return {
            "tool_outputs": [],
            "snippets": metadata.get("snippets", [])
        }

    except Exception as e:
        yield f"Error processing your question: {str(e)}"
        return {
            "tool_outputs": [],
            "snippets": []
        }

def agentic_response(
    question: str,
    chunk_type: str = "sentence",
    vendor: Optional[str] = None,
    top_k: Optional[int] = None,
    fetch_k: Optional[int] = None,
    conversation_history: Optional[List] = None
) -> Iterator[str]:
    """Streaming agentic RAG with LangGraph for intelligent analysis.

    Args:
        question: Current user question
        chunk_type: Type of chunks to retrieve (sentence/review)
        vendor: Optional vendor filter
        top_k: Number of results to return
        fetch_k: Number of candidates for MMR
        conversation_history: DEPRECATED - handled via LangGraph checkpointing

    Yields:
        Text chunks as they are generated by the agent

    Returns:
        Dictionary with keys: tool_outputs (list), snippets (list)
    """
    try:
        graph = get_agent_graph()

        # Get thread_id from Streamlit session (will be set in app.py)
        # This enables conversation persistence across queries
        try:
            import streamlit as st
            if "thread_id" not in st.session_state:
                st.session_state.thread_id = str(uuid.uuid4())
            thread_id = st.session_state.thread_id
        except Exception:
            # Fallback for non-Streamlit usage (e.g., testing)
            thread_id = "default"

        # Create config with thread_id for checkpointing
        config = {"configurable": {"thread_id": thread_id}}

        # Track tool outputs and snippets as they accumulate
        accumulated_tool_outputs = []
        accumulated_snippets = []

        # Stream the graph execution with token-level streaming
        for message_chunk, metadata in graph.stream(
            {
                "messages": [HumanMessage(content=question)],
                "tool_outputs": [],
                "snippets": []
            },
            config=config,
            stream_mode="messages"
        ):
            # Only stream tokens from the agent node (not tools node)
            if metadata.get("langgraph_node") == "agent":
                # Check if chunk has content before accessing
                if hasattr(message_chunk, 'content') and message_chunk.content:
                    yield message_chunk.content

        # Get final state to ensure we have all accumulated data
        final_state = graph.get_state(config)
        if final_state and final_state.values:
            accumulated_tool_outputs = final_state.values.get("tool_outputs", accumulated_tool_outputs)
            accumulated_snippets = final_state.values.get("snippets", accumulated_snippets)

        return {
            "tool_outputs": accumulated_tool_outputs,
            "snippets": accumulated_snippets
        }

    except Exception as e:
        yield f"Error processing your question: {str(e)}"
        return {
            "tool_outputs": [],
            "snippets": []
        }